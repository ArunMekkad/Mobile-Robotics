# HW1: Mobile Robot Kinematics & Control

### EECE 5550: Mobile Robotics (Spring 2025)

**Collaboration Statement:**
#Fill this in per the syllabus, or we will assign a zero to this assignment.
#Did not collaborate with anyone

#References:

#MATLAB - https://www.mathworks.com/help/nav/ug/pure-pursuit-controller.html
#Github - https://github.com/AtsushiSakai/PythonRobotics?tab=readme-ov-file#move-to-a-pose-control for better controller idea

# Installation
This semester, we will use a custom simulator, called `gym-neu-racing`, to develop navigation algorithms. We implemented the basic structure of this simulator for you, and the HW assignments will ask you to implement important functions (e.g., kinematics, sensing, planning, mapping).

To install the simulator, you can use this command (it will download the latest code from GitLab and automatically install it in the environment your Colab notebook runs in):
!pip install git+https://gitlab.com/neu-autonomy/gym-neu-racing.git
Now that the simulator and its dependencies have been installed, you can import the modules you'll need for this assignment:
import gymnasium
import numpy as np
import gym_neu_racing
from gymnasium import spaces
from gym_neu_racing.envs.wrappers import StateFeedbackWrapper
import matplotlib.pyplot as plt
from typing import Callable
You can create an instance of the simulator that you'll build on throughout the assignment:
# Create an instance of the mobile robot simulator we'll use this semester
env = gymnasium.make("gym_neu_racing/NEUEmptyWorld-v0")

# Tell the simulator to directly provide the current state vector (no sensors yet)
env = StateFeedbackWrapper(env)
# Problem 1
## 1a) Run the baseline simulator & generate a plot
Here's an example of how to run the simulator for a few timesteps. In this assignment, we told the simulator to return the true state of the environment (in future assignments, we'll only give noisy sensor data).

In this problem, you should make a plot of the system's state over time. This will be helpful as you debug future parts of the assignment. This part uses the Unicycle kinematic model that's built into the simulator.

**Deliverables**:
- Implement the `plot_path` method. At a minimum, you should plot the (x,y) position over time (the first 2 elements of the state vector). You are welcome to add other capabilities to your `plot_path` function as you go through the assignment (e.g., visualizing the heading angle and/or the goal coordinate), but this is not necessary for this part. You probably should make your axes have the same scale (e.g., using `plt.axis('equal')`).
def plot_path(env: gymnasium.Env, observation_history: list[np.ndarray]) -> None:

    ## Your Implementation Here ##
    x_coords = [obs[0] for obs in observation_history]  #store x-coordinate values in x_coords
    y_coords = [obs[1] for obs in observation_history]  #store y-coordinate values in y_coords

    plt.figure(figsize=(10, 8)) #set figure size
    plt.plot(x_coords, y_coords, 'bo-', markersize=6) #set plot style

    plt.xlabel('X Position') #display label "X Position"
    plt.ylabel('Y Position') #display label "Y Position"
    plt.title('System state plot over time') #display title
    plt.grid(True) #display grid
    plt.axis('equal') #set the same scale for both axes
    plt.show() #display the plot
def dummy_unicycle_controller(current_state: np.ndarray) -> np.ndarray:
  return np.array([3., 0.2]) # [linear speed, angular speed]

def simulate_n_steps(env: gymnasium.Env, num_steps: int, controller: Callable):

  observation_history = []

  obs, _ = env.reset()
  observation_history.append(obs)
  print(f"initial state: {obs}")

  # run the simulator for a few steps and see how the state evolves
  for _ in range(num_steps):
    action = controller(obs)
    obs, _, terminated, _, _ = env.step(action)
    observation_history.append(obs)
    print(f"current state: {obs}")
    if terminated:
      break

  return observation_history
# Tell the simulator to its built-in kinematic model
env.unwrapped.motion_model = gym_neu_racing.motion_models.Unicycle()

# Run the simulation and plot the path taken
num_steps = 10
observation_history = simulate_n_steps(env, num_steps, dummy_unicycle_controller)
plot_path(env, observation_history)
## 1b) Implement a discrete-time unicycle kinematic model

In lecture, we gave the continuous time kinematic model for a unicycle in the form of an ODE (i.e., $\dot{x} = f(x, u)$). In this problem, you should use a simple first-order method (the Euler method) to solve this ODE at discrete timesteps from an initial state, $x[t+dt] = g(x[t], u[t], dt)$.

You should also make sure that your `step` function clips the action so as to respect the control limits. As a hint, you can use `np.clip` and access the control limits with `self.action_space.low` and `self.action_space.high`.

**Deliverables**:
- Implement the `step` method of the `Unicycle` class below
- Generate a plot of the path using your `plot_path` function from above
from gym_neu_racing.motion_models.motion_model import MotionModel


class Unicycle(MotionModel):
    def __init__(self, v_min=0, v_max=1, w_min=-2*np.pi, w_max=2*np.pi):
        self.action_space = spaces.Box(np.array([v_min, w_min]),
                                       np.array([v_max, w_max]),
                                       shape=(2,),
                                       dtype=float)
        super().__init__()

    def step(self, current_state: np.ndarray, action: np.ndarray, dt: float = 0.1) -> np.ndarray:
        # current_state = np.array([x, y, theta])
        # action = np.array([vx, vw])

        ## Your Implementation Here ##
        x, y, theta = current_state #store the current state values in x, y and theta

        v, w = np.clip(action, self.action_space.low, self.action_space.high) #clip the v & w values according to given limits

        #calculate next state using Euler method
        next_x = x + v * np.cos(theta) * dt
        next_y = y + v * np.sin(theta) * dt
        next_theta = theta + w * dt

        next_state = np.array([next_x, next_y, next_theta]) #store updated x, y and theta in next_state

        return next_state

You can tell the simulator (`env`) to use your kinematic model by setting `env.motion_model = YourMotionModel()` (a small caveat is that we have some [wrappers](https://gymnasium.farama.org/api/wrappers/) around the simulator, so you should use the `unwrapped` attribute to directly modify attributes of the base simulator). The simulator will call your motion model's `step` method each timestep (i.e., `self.motion_model.step(self.state.copy(), action)`). To test your kinematic model, you can use the same general idea as before:
# Tell the simulator to use your kinematic model from above
env.unwrapped.motion_model = Unicycle()

# Run the simulation (using the dummy_unicycle controller) and plot the path taken
num_steps = 10
observation_history = simulate_n_steps(env, num_steps, dummy_unicycle_controller)
plot_path(env, observation_history)
## 1c) Implement a discrete-time Simple Car kinematic model

[Section 13.1.2 of Lavelle's book](https://lavalle.pl/planning/ch13.pdf) provides a good explanation of the Simple Car kinematics. In this part, you should implement a first-order discrete time approximation of this model.

**Deliverables**:
- Implement the `SimpleCar` class below
class SimpleCar(MotionModel):
    def __init__(self, L=1., v_min=-1, v_max=1, phi_min=-np.pi/4, phi_max=np.pi/4):
        self.action_space = spaces.Box(np.array([v_min, phi_min]),
                                 np.array([v_max, phi_max]),
                                 shape=(2,),
                                 dtype=float)
        self.L = L
        super().__init__()

    def step(self, current_state: np.ndarray, action: np.ndarray, dt: float = 0.1) -> np.ndarray:
        # current_state = np.array([x, y, theta])
        # action = np.array([u_s, u_phi])

        ## Your Implementation Here ##
        x, y, theta = current_state #store the current state values in x, y and theta

        v_min, v_max = self.action_space.low[0], self.action_space.high[0] #store limits of v
        phi_min, phi_max = self.action_space.low[1], self.action_space.high[1] #store limits of phi

        u_s = np.clip(action[0], v_min,v_max)  #clip Speed
        u_phi = np.clip(action[1], phi_min, phi_max)  #clip steering angle

        #calculate state changes using discrete-time approximation
        dx = u_s * np.cos(theta) * dt
        dy = u_s * np.sin(theta) * dt
        dtheta = (u_s / self.L) * np.tan(u_phi) * dt

        #store updated x, y and theta in next_x, next_y and next_theta
        next_x = x + dx
        next_y = y + dy
        next_theta = theta + dtheta

        next_state = np.array([next_x, next_y, next_theta])

        return next_state
def dummy_car_controller(current_state: np.ndarray) -> np.ndarray:
  return np.array([0.5, -0.1]) # [linear speed, steering angle]
# Tell the simulator to use your kinematic model from above
env.unwrapped.motion_model = SimpleCar(L=0.5)

# Run the simulation (using the dummy car controller) and plot the path taken
num_steps = 10
observation_history = simulate_n_steps(env, num_steps, dummy_car_controller)
plot_path(env, observation_history)
# Problem 2: Controlling the Robot to Reach a Goal Coordinate
## 2a) Open-Loop Control
Using the `Unicycle` model you implemented above, you will now command the system to reach a goal position. Start with open-loop control, i.e., measure the intial state (returned by `env.reset`) and then calculate a sequence of actions to apply.

After that sequence is computed, run the simulator forward for several steps, and at each step, simply grab the corresponding control value from that pre-computed sequence. You can use this example `open_loop_control_policy` implementation as a first try.

Your job is to implement a smarter `open_loop_control_policy` that gets the system to reach (close) to the goal.

Remember: you are only provided with the initial state, goal position, and the maximum number of steps allowed -- you can't access the system's state once it has started moving! That would be closed-loop control, which we'll implement next :)

**Deliverables**:
- Implement the `open_loop_control_policy`, which should produce a sequence of control commands that will lead to the goal. This can be a very simple hard-coded policy (e.g., always send the same v, w)
- Generate 1 plot that shows your open-loop controller drives the noise-free system to the goal (it only has to work for 1 test case). Include the path taken and the start/goal coordinates in your plot.
- Generate 1 plot that shows your open-loop controller running on the noisy system (it's ok if it doesn't work on this system). Include the path taken and the start/goal coordinates in your plot.
def open_loop_control_policy(init_state: np.ndarray, goal: np.ndarray, num_steps: int = 10) -> list[np.ndarray]:

        ## Your Implementation Here ##
        control_sequence = []
        x, y, theta = init_state #store the initial state values in x, y and theta
        goal_x, goal_y = goal #store goal coordinate values in goal_x and goal_y

        dx = goal_x - x #difference in x-coordinates between goal and initial position
        dy = goal_y - y #difference in y-coordinates between goal and initial position
        distance_to_goal = np.sqrt(dx**2 + dy**2) #euclidean distance to the goal
        goal_heading = np.arctan2(dy, dx) #angle between initial position and goal

        # Calculate fixed linear and angular velocities
        v = distance_to_goal / (num_steps *0.05)  #velocity = distance / num_steps * time interval(hard-coded)
        w = (goal_heading - theta) / (num_steps * 0.05)  #angular speed = difference in angle / num_steps * time interval(hard-coded)

        # Generate control sequence
        control_sequence = [[v, w] for _ in range(num_steps)] #create list control_sequence storing control inputs v & w

        return control_sequence
np.random.seed(0)
max_num_steps = 200

env.unwrapped.motion_model = Unicycle()
obs, _ = env.reset() # initial observation of the system's state vector
observation_history = [obs]

# query your open loop controller based on the initial state of the system
control_sequence = open_loop_control_policy(obs, env.unwrapped.goal, num_steps=max_num_steps)

# Now that we've computed the control sequence, run the system forward until
# we reach max_num_steps or the system's state is close to the goal
for step in range(max_num_steps):
  action = control_sequence[step]
  obs, _, terminated, _, _ = env.step(action)
  observation_history.append(obs)
  if terminated:
    break

# Generate a simple plot of the path taken
plot_path(env, observation_history)

if terminated:
  print('Success! Your open-loop controller drove the system to the goal.')
else:
  print('Your open-loop controller did not successfully drive the system to the goal.')
Now, let's see what happens if we try that same `control_sequence` on a version of the environment where the wheels are a little bit slippery, so our simple unicycle model does not describe the true system perfectly.


class NoisyUnicycle(Unicycle):
  """Same as Unicycle but add process noise at each step."""
  def __init__(self,
               process_noise_limits=np.array([0.1, 0.1, 0.5]),
               v_min=0, v_max=1, w_min=-2*np.pi, w_max=2*np.pi):
    self.process_noise_limits = process_noise_limits
    super().__init__(v_min=v_min, v_max=v_max, w_min=w_min, w_max=w_max)

  def step(self, current_state, action, dt=0.1) -> np.ndarray:

    """Add process noise to the parent kinematics model."""
    next_state = super().step(current_state, action, dt=dt)
    perturbed_next_state = next_state + np.random.uniform(low=-self.process_noise_limits, high=self.process_noise_limits)

    return perturbed_next_state
This class we wrote for you will make it so you can't perfectly measure the system's state. It adds random noise to the `obs` that the `env` returns at each step:
from gymnasium import ObservationWrapper

class NoisyStateFeedbackWrapper(ObservationWrapper):
  def __init__(self, env, sensor_noise_limits=np.array([0.05, 0.05, 0.2])):
    self.sensor_noise_limits = sensor_noise_limits
    super().__init__(env)

  def observation(self, observation: dict) -> np.ndarray:
    obs = observation["state"] + np.random.uniform(low=-self.sensor_noise_limits, high=self.sensor_noise_limits)
    return obs
Now, run your open loop controller on a Unicycle with both process & sensor noise. Your open-loop control sequence probably does not drive the system to the goal anymore. This is to be expected! It's like trying to park a car in a parking spot with your eyes closed. We will implement a much smarter strategy next.
np.random.seed(0)
max_num_steps = 200

env = gymnasium.make("gym_neu_racing/NEUEmptyWorld-v0")

# Add both sensor noise & process noise to make the problem more challenging
env = NoisyStateFeedbackWrapper(env)
env.unwrapped.motion_model = NoisyUnicycle()

obs, _ = env.reset() # initial observation of the system's state vector
observation_history = [obs]

# query your open loop controller based on the initial state of the system
control_sequence = open_loop_control_policy(obs, env.unwrapped.goal, num_steps=max_num_steps)

# Now that we've computed the control sequence, run the system forward until
# we reach max_num_steps or the system's state is close to the goal
for step in range(max_num_steps):
  action = control_sequence[step]
  obs, _, terminated, _, _ = env.step(action)
  observation_history.append(obs)
  if terminated:
    break

# Generate a simple plot of the path taken
plot_path(env, observation_history)

if terminated:
  print('Success! Your open-loop controller drove the system to the goal.')
else:
  print('Your open-loop controller did not successfully drive the system to the goal.')
## 2b) Closed-Loop Control: Pure Pursuit Algorithm
One of the most powerful ideas in robotic control is *feedback*. In this next part, we will implement a controller that measures the system's state at each step and uses that measurement to compute an action. This way, if the system ended up in a slightly different state than was expected from the simple model, the measurement will contain information about this discrepancy. Feedback control is often a great way to compensate for imperfect knowledge of the system you are working with.

Your job is to implement the `PurePursuitController` class with a `get_action` method that takes in the current `obs` and the `env.unwrapped.goal` coordinate (expressed in the global frame), and outputs one `action` = $[v, \omega]$. This `get_action` method will be called at each step, so there is no need to calculate a whole sequence of actions. The controller that you implement should be capable of driving the system to the goal.
class PurePursuitController:
  def __init__(self, L=3.):
    """Store any hyperparameters here."""
    self.L = L

  def get_action(self, obs: np.ndarray, goal: np.ndarray) -> np.ndarray:

        """Your implementation goes here"""
        x, y, theta = obs
        goal_x, goal_y = goal

        linear_speed = 8.25

        dx = goal_x - x #distance the robot should travel along x-axis to reach the goal
        dy = goal_y - y #distance the robot should travel along y-axis to reach the goal

        # 1. Transform goal to robot's local frame
        local_y = - dx * np.sin(theta) + dy * np.cos(theta)

        lookahead_distance = self.L
        angular_speed =  linear_speed*2 * local_y / (lookahead_distance**2)

        return np.array([linear_speed, angular_speed])
This `validation` function may come in handy for debugging/testing your controller in some random scenarios
def validation(controller, motion_model=Unicycle, sensor_model=StateFeedbackWrapper, plot=False, num_runs=100, max_num_steps_per_run=200):
  success_per_run = np.empty((num_runs,), dtype=bool)
  num_steps_per_run = np.empty((num_runs,))
  for i in range(num_runs):
    np.random.seed(i)
    env = gymnasium.make("gym_neu_racing/NEUEmptyWorld-v0")

    env = sensor_model(env)
    env.unwrapped.motion_model = motion_model()

    obs, _ = env.reset()
    observation_history = [obs]
    for step in range(max_num_steps_per_run):
      action = controller.get_action(obs, env.unwrapped.goal)
      obs, _, terminated, _, _ = env.step(action)
      observation_history.append(obs)
      if terminated:
        break

    success_per_run[i] = terminated
    num_steps_per_run[i] = step
    if plot: plot_path(env, observation_history)

  avg_success = np.mean(success_per_run)
  avg_steps = np.mean(num_steps_per_run)
  return avg_success, avg_steps
For example, you can run your controller on 3 random cases and plot the resulting paths:
pure_pursuit_controller = PurePursuitController(L=3.)
validation(controller=pure_pursuit_controller, num_runs=3, plot=True)
Once you're happy with your controller, you can run it against a larger validation test suite. This function will return the success rate (between 0 and 1) and average number of steps it takes your system to reach the goal.
pure_pursuit_controller = PurePursuitController()
avg_success, avg_steps = validation(controller=pure_pursuit_controller)
print("Success rate:", avg_success)
print("Avg. Number of Steps:", avg_steps)
Now we have a closed-loop controller that will drive the system to the goal.

Let's see if that controller can handle the noisy version of the Unicycle model:
pure_pursuit_controller = PurePursuitController()
avg_success, avg_steps = validation(controller=pure_pursuit_controller, motion_model=NoisyUnicycle, sensor_model=NoisyStateFeedbackWrapper, num_runs=5, plot=True)
print("Success rate:", avg_success)
print("Avg. Number of Steps:", avg_steps)
Hopefully the answer is yes! Even though the vehicle doesn't perfectly follow the curve that the pure pursuit algorithm had in mind, the feedback control strategy compensates for the imperfect knowledge of the dynamics and drives the system to the goal.
**Deliverables**:
- Implement the `PurePursuitController` class using the ideas presented in Lecture 2. For partial credit, get a success rate of 100%. For full credit, get a success rate of 100% with an average number of steps below 82
- Include plots and text (one paragraph maximum) below describing your observations about the impact of hyperparameters on your algorithm.

Note: The autograder will simply instantiate your controller by running `PurePursuitController()`, so make sure that any hyperparameters default to the values you would like (e.g., use keyword arguments with default values for your lookahead distance).
**Conclusion:**

The pure pursuit controller was successfully designed to get a success rate of 1.0 in both test and validation sets with an average number of steps of 72 and 79.38 respectively. This controller can be used effectively in cases where the linear speed of robot is given (maintain a constant speed) and coordinates of the goal is given. The lookahead distance L is set as a hyperparamter and it gave out an optimal result when it was set at 3. Since the linear speed is constant, it can also be considered as a hyperparamter, since tuning of linear speed impacts the success and average number of steps. However it was not added to the hyperparamter section within the init method definition so as to not contradict with the autograder(hard coded in the step definition). The controller performed the best within this setup when the linear speed was set at 8.25 (it performed the same when linear speed was set in the range of (8.25,9.25)). However, the introduction of noise impacts the controller since the robot does not follow the heading and instead it follows a lookahead point. A better controller was also designed that follows the heading towards goal and adjusts the linear speed according to the curvature in the below section. A proportional control was implemented to change the angular speed proportional to the angle of robot with respect to the goal.
## 2c) Extra Credit: Make your algorithm even better
Prof. Everett's simple pure pursuit implementation used an average number of steps of 79.44 on the validation set. You can earn 1 extra credit point if you upload a second implementation to Gradescope achieves below 80 steps on average (while also achieving 100% success). You can earn additional extra credit for having the best implementation in the class according to the Gradescope leaderboard.

You can try different strategies than just pure pursuit. Whichever control strategy you end up using, please use the `BetterController` class below for this part, which has the same class/function signatures as before.

Note: there will be a test suite that is similar but not identical to the validation method above to ensure your algorithm doesn't overfit to specific cases.
class BetterController:
    def __init__(self, linear_speed=9.6, Kp = 5.6, ls_adj = 5.5):
        self.linear_speed = linear_speed
        self.Kp = Kp
        self.ls_adj = ls_adj

    def get_action(self, obs: np.ndarray, goal: np.ndarray) -> np.ndarray:
        x, y, theta = obs
        goal_x, goal_y = goal

        linear_speed = self.linear_speed #constant linear speed
        Kp = self.Kp #proportional gain
        ls_adj = self.ls_adj #gain for linear speed adjustment

        #calculate the difference in x and y
        local_x = (goal_x - x) * np.cos(theta) + (goal_y - y) * np.sin(theta)
        local_y = -(goal_x - x) * np.sin(theta) + (goal_y - y) * np.cos(theta)

        #calculate angle in the local frame
        angle_to_goal = np.arctan2(local_y, local_x)

        angular_speed = Kp * angle_to_goal  #proportional control

        linear_speed = linear_speed - ls_adj * abs(angle_to_goal)  #linear speed adjustment
        return np.array([linear_speed, angular_speed])

better_controller = BetterController()
avg_success, avg_steps = validation(controller=better_controller)
print("Success rate:", avg_success)
print("Avg. Number of Steps:", avg_steps)
better_controller = BetterController()
avg_success, avg_steps = validation(controller=better_controller, motion_model=NoisyUnicycle, sensor_model=NoisyStateFeedbackWrapper, num_runs=5, plot=True)
print("Success rate:", avg_success)
print("Avg. Number of Steps:", avg_steps)
